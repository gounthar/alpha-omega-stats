{
    "checksum": "61ee2d3637caaeb523db9127147611aabe5e7174dbe552ef396dc1fbc8f417da",
    "data": {
    "id": "0d9f30ba-5df2-4ba4-a441-707c913d99b6",
    "createdAt": 1743511800956,
    "updatedAt": 1743511813681,
    "title": "Local Success Builds Sheet",
    "messages": [
        {
            "id": "5d57fbbb-d438-4567-a861-735ab98347cc",
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Could we leverage the data found in \"successful_builds.csv \" within \"upload_to_sheets.py\" by creating a new sheet in the Google document?"
                },
                {
                    "type": "text",
                    "text": "This file lists the PRs that don't build successfully on the Jenkins infrastructure, but build correctly locally, with the JDK they built with cited."
                },
                {
                    "type": "text",
                    "text": "What could be a short but good name for this sheet?"
                },
                {
                    "type": "text",
                    "text": "Thanks."
                }
            ],
            "context": {
                "currentFile": {
                    "path": "upload_to_sheets.py",
                    "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "import gspread\nimport gspread.exceptions  # Ensure exceptions are properly referenced if not already imported\nfrom google.oauth2.service_account import Credentials\nimport json\nimport time\nimport logging\nfrom datetime import datetime\nimport sys\nimport re\nfrom time import sleep\nimport random\nimport os\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogging.info(\"Starting script...\")\n\ndef get_backoff_duration(attempt, base_delay=5, max_delay=300):\n    \"\"\"\n    Calculate exponential backoff duration with jitter.\n    \n    Args:\n        attempt: The current retry attempt number (0-based)\n        base_delay: The base delay in seconds (default: 5)\n        max_delay: Maximum delay in seconds (default: 300)\n    \n    Returns:\n        Delay duration in seconds\n    \"\"\"\n    delay = min(base_delay * (2 ** attempt), max_delay)\n    jitter = random.uniform(0, 0.1 * delay)  # Add up to 10% jitter\n    return delay + jitter\n\ndef handle_google_api_error(e, attempt, max_retries):\n    \"\"\"\n    Handle different types of Google Sheets API errors.\n    \n    Returns:\n        tuple: (should_retry, wait_time)\n    \"\"\"\n    error_str = str(e).lower()\n    \n    # Rate limit errors\n    if \"429\" in error_str or \"quota\" in error_str:\n        wait_time = get_backoff_duration(attempt)\n        logging.warning(f\"Rate limit exceeded. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Backend errors (500s)\n    if any(code in error_str for code in [\"500\", \"502\", \"503\", \"504\"]):\n        wait_time = get_backoff_duration(attempt, base_delay=10)\n        logging.warning(f\"Backend error encountered. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Authorization errors\n    if \"401\" in error_str or \"403\" in error_str:\n        logging.error(\"Authorization error. Please check your credentials.\")\n        return False, 0\n    \n    # Invalid request errors\n    if \"400\" in error_str:\n        logging.error(\"Invalid request error. Please check your input data.\")\n        return False, 0\n    \n    # Default case - retry with standard backoff\n    wait_time = get_backoff_duration(attempt)\n    logging.warning(f\"Unexpected error. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n    return True, wait_time\n\ndef retry_with_backoff(func, max_retries=5, initial_delay=5):\n    \"\"\"\n    Retry a function with exponential backoff and improved error handling.\n    \"\"\"\n    last_error = None\n    \n    for attempt in range(max_retries):\n        try:\n            return func()\n        except gspread.exceptions.APIError as e:\n            should_retry, wait_time = handle_google_api_error(e, attempt, max_retries)\n            if not should_retry or attempt == max_retries - 1:\n                raise\n            time.sleep(wait_time)\n            last_error = e\n        except gspread.exceptions.SpreadsheetNotFound:\n            logging.error(\"Spreadsheet not found. Please check the spreadsheet ID or name.\")\n            raise\n        except gspread.exceptions.WorksheetNotFound:\n            logging.error(\"Worksheet not found. Please check the worksheet name.\")\n            raise\n        except Exception as e:\n            logging.error(f\"Unexpected error: {str(e)}\")\n            raise\n    \n    if last_error:\n        raise last_error\n    return None\n\ndef sanitize_sheet_name(title, max_length=100):\n    \"\"\"\n    Sanitize a title to be used as a Google Sheets worksheet name.\n    - Removes invalid characters\n    - Truncates to max_length\n    - Ensures uniqueness by adding a counter if needed\n    \"\"\"\n    # Remove invalid characters and replace spaces with underscores\n    sanitized = re.sub(r'[\\[\\]\\\\*?/:]', '', title)\n    sanitized = sanitized.replace(' ', '_')\n    \n    # Truncate to max_length\n    if len(sanitized) > max_length:\n        # Keep the first part of the title and add a hash of the full title\n        hash_suffix = str(hash(title))[-8:]\n        sanitized = sanitized[:max_length-9] + '_' + hash_suffix\n    \n    return sanitized\n\ndef update_sheet_with_retry(sheet, data, range_name=\"A1\"):\n    \"\"\"\n    Update a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def update():\n        try:\n            # First clear the entire sheet\n            sheet.clear()\n            time.sleep(1)  # Add a small delay after clearing\n            \n            # Validate data before updating\n            if not data or not isinstance(data, (list, tuple)):\n                raise ValueError(\"Invalid data format. Expected non-empty list or tuple.\")\n            \n            # Then update with new data\n            sheet.update(range_name=range_name, values=data, value_input_option=\"USER_ENTERED\")\n            logging.info(f\"Successfully updated sheet with {len(data)} rows of data\")\n            time.sleep(2)  # Add delay between operations\n        except gspread.exceptions.APIError as e:\n            logging.error(f\"API error during sheet update: {str(e)}\")\n            raise\n        except Exception as e:\n            logging.error(f\"Error during sheet update: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(update)\n\ndef format_sheet_with_retry(sheet, range_name, format_dict):\n    \"\"\"\n    Format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def format():\n        try:\n            sheet.format(range_name, format_dict)\n            logging.info(f\"Successfully formatted range {range_name}\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during sheet formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(format)\n\ndef batch_format_with_retry(sheet, format_requests):\n    \"\"\"\n    Batch format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def batch_format():\n        try:\n            sheet.batch_format(format_requests)\n            logging.info(f\"Successfully applied {len(format_requests)} format requests\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during batch formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(batch_format)\n\ndef validate_pr_data(pr):\n    \"\"\"\n    Validate PR data structure and required fields.\n    Returns (is_valid, error_message)\n    \"\"\"\n    required_fields = {\n        \"title\": str,\n        \"repository\": str,\n        \"number\": (int, str),  # Allow both int and str\n        \"state\": str,\n        \"createdAt\": str,\n        \"updatedAt\": str,\n        \"checkStatus\": (str, type(None))  # Allow string or None\n    }\n    \n    for field, field_type in required_fields.items():\n        if field not in pr:\n            return False, f\"Missing required field: {field}\"\n        if not isinstance(pr[field], field_type):\n            if isinstance(field_type, tuple):\n                if not any(isinstance(pr[field], t) for t in field_type):\n                    return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n            else:\n                return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n    \n    # Validate state values\n    if pr[\"state\"] not in [\"OPEN\", \"CLOSED\", \"MERGED\"]:\n        return False, f\"Invalid state value: {pr['state']}\"\n    \n    # Validate date formats\n    try:\n        datetime.fromisoformat(pr[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        datetime.fromisoformat(pr[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n    except ValueError as e:\n        return False, f\"Invalid date format: {e}\"\n    \n    return True, None\n\ndef group_prs_by_title(prs):\n    \"\"\"\n    Group PRs by title and calculate statistics.\n    \"\"\"\n    title_groups = {}\n    for pr in prs:\n        title = pr[\"title\"]\n        if title not in title_groups:\n            title_groups[title] = {\n                \"title\": title,\n                \"prs\": [],\n                \"open\": 0,\n                \"closed\": 0,\n                \"merged\": 0\n            }\n        \n        group = title_groups[title]\n        group[\"prs\"].append(pr)\n        if pr[\"state\"] == \"OPEN\":\n            group[\"open\"] += 1\n        elif pr[\"state\"] == \"CLOSED\":\n            group[\"closed\"] += 1\n        elif pr[\"state\"] == \"MERGED\":\n            group[\"merged\"] += 1\n    \n    return list(title_groups.values())\n\ndef process_consolidated_data(consolidated_file):\n    \"\"\"\n    Process consolidated PR data and validate it.\n    Returns (grouped_prs, failing_prs, errors)\n    \"\"\"\n    errors = []\n    valid_prs = []\n    \n    try:\n        with open(consolidated_file) as f:\n            prs = json.load(f)\n    except FileNotFoundError:\n        return None, None, [f\"File {consolidated_file} not found.\"]\n    except json.JSONDecodeError:\n        return None, None, [f\"Error decoding {consolidated_file}.\"]\n    \n    if not isinstance(prs, list):\n        return None, None, [\"Consolidated data must be a list of PRs.\"]\n    \n    # Validate each PR\n    for i, pr in enumerate(prs):\n        is_valid, error = validate_pr_data(pr)\n        if is_valid:\n            valid_prs.append(pr)\n        else:\n            errors.append(f\"PR at index {i}: {error}\")\n    \n    if not valid_prs:\n        return None, None, [\"No valid PRs found in consolidated data.\"]\n    \n    # Group valid PRs by title\n    grouped_prs = group_prs_by_title(valid_prs)\n    \n    # Extract failing PRs\n    failing_prs = [\n        {\n            \"title\": pr[\"title\"],\n            \"url\": f\"https://github.com/{pr['repository']}/pull/{pr['number']}\",\n            \"status\": pr[\"checkStatus\"]\n        }\n        for pr in valid_prs\n        if pr[\"state\"] == \"OPEN\" and pr[\"checkStatus\"] in [\"ERROR\", \"FAILURE\"]\n    ]\n    \n    return grouped_prs, failing_prs, errors\n\n# Main execution\nif len(sys.argv) != 3:\n    print(\"Usage: python3 upload_to_sheets.py <consolidated-prs-json-file> <failing-prs-error-state>\")\n    sys.exit(1)\n\nCONSOLIDATED_FILE = sys.argv[1]\nFAILING_PRS_ERROR = sys.argv[2].lower() == 'true'\n\n# Process consolidated data\ngrouped_prs, failing_prs, errors = process_consolidated_data(CONSOLIDATED_FILE)\n\nif errors:\n    for error in errors:\n        logging.error(error)\n    if not grouped_prs:  # Fatal errors\n        sys.exit(1)\n    else:  # Non-fatal errors\n        logging.warning(\"Some PRs were invalid but processing will continue.\")\n\n# Define the scope\nscope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n\n# Add your service account credentials\n# Check for environment variable first, then fall back to local file\ncredentials_file = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', 'concise-complex-344219-062a255ca56f.json')\ntry:\n    creds = Credentials.from_service_account_file(credentials_file, scopes=scope)\n    logging.info(f\"Using credentials from: {credentials_file}\")\nexcept FileNotFoundError:\n    # For GitHub Actions, try the file we created in the workflow\n    fallback_file = 'google-credentials.json'\n    logging.info(f\"Credentials file {credentials_file} not found, trying {fallback_file}\")\n    creds = Credentials.from_service_account_file(fallback_file, scopes=scope)\n\n# Authorize the client\nclient = gspread.authorize(creds)\n\n# Open the Google Sheet by name or ID\nspreadsheet = client.open(\"Jenkins PR Tracker\")  # or use client.open_by_key(\"YOUR_SHEET_ID\")\n\n# Create a summary sheet\ntry:\n    summary_sheet = spreadsheet.worksheet(\"Summary\")\n    logging.info(\"Summary sheet already exists. Updating it...\")\nexcept gspread.exceptions.WorksheetNotFound:\n    logging.info(\"Creating new Summary sheet...\")\n    summary_sheet = spreadsheet.add_worksheet(title=\"Summary\", rows=100, cols=10)\n\n# Prepare summary data\ntotal_prs = 0\nopen_prs = 0\nclosed_prs = 0\nmerged_prs = 0\nplugin_stats = {}\nearliest_date = None\nlatest_date = None\n\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    total_prs += len(prs)\n    open_prs += pr[\"open\"]\n    closed_prs += pr[\"closed\"]\n    merged_prs += pr[\"merged\"]\n\n    # Plugin-specific stats\n    plugin_stats[title] = {\n        \"total\": len(prs),\n        \"open\": pr[\"open\"],\n        \"closed\": pr[\"closed\"],\n        \"merged\": pr[\"merged\"]\n    }\n\n    # Find the earliest and latest dates\n    for p in prs:\n        created_at = datetime.fromisoformat(p[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        updated_at = datetime.fromisoformat(p[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n\n        if earliest_date is None or created_at < earliest_date:\n            earliest_date = created_at\n        if latest_date is None or updated_at > latest_date:\n            latest_date = updated_at\n\n# Calculate percentages\nopen_percentage = (open_prs / total_prs) * 100 if total_prs > 0 else 0\nclosed_percentage = (closed_prs / total_prs) * 100 if total_prs > 0 else 0\nmerged_percentage = (merged_prs / total_prs) * 100 if total_prs > 0 else 0\n\n# Prepare summary data for the sheet\nsummary_data = [\n    [\"PR Date Range\", f\"{earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\", \"\", \"\", \"\", \"\"],\n    [\"Overall PR Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Total PRs\", total_prs, \"\", \"\", \"\", \"\"],\n    [\"Open PRs\", open_prs, f\"{open_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Closed PRs\", closed_prs, f\"{closed_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Merged PRs\", merged_prs, f\"{merged_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin-Specific Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin\", \"Total PRs\", \"Open PRs\", \"Closed PRs\", \"Merged PRs\", \"Link to Sheet\"]\n]\n\n# Add plugin-specific stats and links to individual sheets\nfor plugin, stats in plugin_stats.items():\n    sheet_name = sanitize_sheet_name(plugin)\n    if not sheet_name:\n        logging.error(f\"Invalid sheet name generated for plugin '{plugin}'. Skipping sheet creation.\")\n        continue\n\n    try:\n        plugin_sheet = spreadsheet.worksheet(sheet_name)\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n        plugin_sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n    link = f'=HYPERLINK(\"#gid={plugin_sheet.id}\"; \"{plugin}\")'\n    summary_data.append([\n        plugin,\n        stats[\"total\"],\n        stats[\"open\"],\n        stats[\"closed\"],\n        stats[\"merged\"],\n        link\n    ])\n\n# Update the summary sheet\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Reorder sheets to make the Summary sheet first\nsheets = spreadsheet.worksheets()\nif sheets[0].title != \"Summary\":\n    summary_sheet_index = next((i for i, sheet in enumerate(sheets) if sheet.title == \"Summary\"), None)\n    if summary_sheet_index is not None:\n        spreadsheet.reorder_worksheets(\n            [sheets[summary_sheet_index]] + [sheet for i, sheet in enumerate(sheets) if i != summary_sheet_index])\n\n# Get the Summary sheet ID for the \"Back to Summary\" link\nsummary_sheet_id = summary_sheet.id\n\n# Format the summary sheet\nformat_sheet_with_retry(summary_sheet, \"A1:F1\", {\n    \"textFormat\": {\n        \"bold\": True\n    },\n    \"backgroundColor\": {\n        \"red\": 0.9,  # Light gray background\n        \"green\": 0.9,\n        \"blue\": 0.9,\n        \"alpha\": 1.0\n    },\n    \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n})\n\n# Create a new sheet for failing PRs\nif failing_prs and isinstance(failing_prs, list):\n    try:\n        failing_prs_sheet = spreadsheet.worksheet(\"Failing PRs\")\n        logging.info(\"Failing PRs sheet already exists. Updating it...\")\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(\"Creating new Failing PRs sheet...\")\n        failing_prs_sheet = spreadsheet.add_worksheet(title=\"Failing PRs\", rows=100, cols=10)\n\n    # Prepare the data for the failing PRs sheet\n    failing_prs_data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Title\", \"URL\", \"Status\"]\n    ]\n\n    # Process each failing PR\n    for pr in failing_prs:\n        failing_prs_data.append([\n            pr[\"title\"],\n            f'=HYPERLINK(\"{pr[\"url\"]}\"; \"{pr[\"url\"]}\")',\n            pr[\"status\"]\n        ])\n\n    # Clear the sheet and update it with the new data\n    update_sheet_with_retry(failing_prs_sheet, failing_prs_data)\n\n    # Format the column titles (bold font and background color)\n    format_sheet_with_retry(failing_prs_sheet, \"A3:C3\", {  # Format only the column titles (row 3)\n        \"textFormat\": {\n            \"bold\": True\n        },\n        \"backgroundColor\": {\n            \"red\": 0.9,  # Light gray background\n            \"green\": 0.9,\n            \"blue\": 0.9,\n            \"alpha\": 1.0\n        },\n        \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n    })\n\n    # Calculate failing PRs count\n    failing_prs_count = len(failing_prs)\nelse:\n    failing_prs_count = 0\n    logging.warning(\"No failing PRs found in the data.\")\n\n# Add a link to the \"Failing PRs\" sheet in the \"Summary\" sheet and include the count\nif failing_prs and 'failing_prs_sheet' in locals():\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", f'=HYPERLINK(\"#gid={failing_prs_sheet.id}\"; \"Failing PRs\")'])\nelse:\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", \"No failing PRs found\"])\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Iterate through each PR group and create a new sheet for each title\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    sheet_name = sanitize_sheet_name(title)\n\n    # Prepare the data for the sheet\n    data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [title, \"\", \"\", \"\", \"\"],  # Add title without label\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Repository\", \"PR Number\", \"State\", \"Created At\", \"Updated At\"]\n    ]\n    for p in prs:\n        # Add hyperlinks to the Repository and PR Number columns\n        repo_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}\"; \"{p[\"repository\"]}\")'\n        pr_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}/pull/{p[\"number\"]}\"; \"{p[\"number\"]}\")'\n        data.append([repo_link, pr_link, p[\"state\"], p[\"createdAt\"], p[\"updatedAt\"]])\n\n    try:\n        # Check if a sheet with the same title already exists\n        try:\n            sheet = spreadsheet.worksheet(sheet_name)\n            logging.info(f\"Sheet '{sheet_name}' already exists. Updating it...\")\n        except gspread.exceptions.WorksheetNotFound:\n            # Create a new sheet if it doesn't exist\n            logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n            sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n        # Update sheet with retry logic\n        update_sheet_with_retry(sheet, data)\n\n        # Format the \"Back to Summary\" row - only format the first cell\n        format_sheet_with_retry(sheet, \"A1\", {\n            \"textFormat\": {\n                \"fontSize\": 10\n            }\n        })\n\n        # Format the title row - only format the first cell\n        format_sheet_with_retry(sheet, \"A2\", {\n            \"textFormat\": {\n                \"bold\": True,\n                \"fontSize\": 10\n            },\n            \"horizontalAlignment\": \"LEFT\"\n        })\n\n        # Format the column titles\n        format_sheet_with_retry(sheet, \"A4:E4\", {\n            \"textFormat\": {\n                \"bold\": True\n            },\n            \"backgroundColor\": {\n                \"red\": 0.9,\n                \"green\": 0.9,\n                \"blue\": 0.9,\n                \"alpha\": 1.0\n            },\n            \"horizontalAlignment\": \"CENTER\"\n        })\n\n        # Apply conditional formatting based on PR state\n        format_requests = []\n        for row_idx, p in enumerate(prs, start=5):  # Start from row 5 (skip header, title, and \"Back to Summary\" rows)\n            color = {\n                \"MERGED\": {\"red\": 0.0, \"green\": 1.0, \"blue\": 0.0, \"alpha\": 1.0},\n                \"OPEN\": {\"red\": 1.0, \"green\": 0.5, \"blue\": 0.0, \"alpha\": 1.0},\n                \"CLOSED\": {\"red\": 1.0, \"green\": 0.0, \"blue\": 0.0, \"alpha\": 1.0},\n            }.get(p[\"state\"], {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0, \"alpha\": 1.0})\n\n            format_requests.append({\n                \"range\": f\"A{row_idx}:E{row_idx}\",\n                \"format\": {\n                    \"backgroundColor\": color\n                }\n            })\n\n        # Apply all formatting requests in a single batch\n        if format_requests:\n            batch_format_with_retry(sheet, format_requests)\n\n        # Add a longer delay between sheets to avoid rate limits\n        time.sleep(10)  # Increased delay between sheets\n\n    except gspread.exceptions.APIError as e:\n        logging.error(f\"Failed to update sheet '{sheet_name}': {e}\")\n        continue\n\nlogging.info(\"Data has been uploaded to Google Sheets.\")\n"
                },
                "codebaseEnabled": true,
                "ragFiles": [
                    {
                        "path": "Dockerfile",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\Dockerfile",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/spreadsheet.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\spreadsheet.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/worksheet.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\worksheet.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/pip/_vendor/urllib3/contrib/appengine.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\pip\\_vendor\\urllib3\\contrib\\appengine.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_credentials_base.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_credentials_base.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_helpers.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_helpers.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_credentials_async.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_credentials_async.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/metrics.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\metrics.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_exponential_backoff.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_exponential_backoff.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_refresh_worker.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_refresh_worker.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/aio/__init__.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\aio\\__init__.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/crypt/base.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\crypt\\base.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/transport/_custom_tls_signer.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\transport\\_custom_tls_signer.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/transport/_mtls_helper.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\transport\\_mtls_helper.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/webauthn_handler_factory.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\webauthn_handler_factory.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/utils.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\utils.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/webauthn_types.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\webauthn_types.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/sts.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\sts.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/client.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\client.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "test-pr-builds.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\test-pr-builds.sh",
                        "sources": []
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": []
                    },
                    {
                        "path": "group-prs.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\group-prs.sh",
                        "sources": []
                    },
                    {
                        "path": "update-daily.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\update-daily.sh",
                        "sources": []
                    },
                    {
                        "path": "generate-report.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\generate-report.sh",
                        "sources": []
                    },
                    {
                        "path": "analyze-junit5-prs.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\analyze-junit5-prs.sh",
                        "sources": []
                    },
                    {
                        "path": "data\\consolidated\\test_results.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\test_results.csv",
                        "sources": []
                    },
                    {
                        "path": "data\\consolidated\\successful_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\successful_builds.csv",
                        "sources": []
                    },
                    {
                        "path": "data/consolidated/failed_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\failed_builds.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "data/consolidated/successful_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\successful_builds.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "data/consolidated/test_results.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\test_results.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "update-daily.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\update-daily.sh",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "opened_tab"
                        ]
                    }
                ],
                "rerankedFiles": [],
                "usedFilePaths": [
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "opened_tab"
                        ],
                        "content": "import gspread\nimport gspread.exceptions  # Ensure exceptions are properly referenced if not already imported\nfrom google.oauth2.service_account import Credentials\nimport json\nimport time\nimport logging\nfrom datetime import datetime\nimport sys\nimport re\nfrom time import sleep\nimport random\nimport os\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogging.info(\"Starting script...\")\n\ndef get_backoff_duration(attempt, base_delay=5, max_delay=300):\n    \"\"\"\n    Calculate exponential backoff duration with jitter.\n    \n    Args:\n        attempt: The current retry attempt number (0-based)\n        base_delay: The base delay in seconds (default: 5)\n        max_delay: Maximum delay in seconds (default: 300)\n    \n    Returns:\n        Delay duration in seconds\n    \"\"\"\n    delay = min(base_delay * (2 ** attempt), max_delay)\n    jitter = random.uniform(0, 0.1 * delay)  # Add up to 10% jitter\n    return delay + jitter\n\ndef handle_google_api_error(e, attempt, max_retries):\n    \"\"\"\n    Handle different types of Google Sheets API errors.\n    \n    Returns:\n        tuple: (should_retry, wait_time)\n    \"\"\"\n    error_str = str(e).lower()\n    \n    # Rate limit errors\n    if \"429\" in error_str or \"quota\" in error_str:\n        wait_time = get_backoff_duration(attempt)\n        logging.warning(f\"Rate limit exceeded. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Backend errors (500s)\n    if any(code in error_str for code in [\"500\", \"502\", \"503\", \"504\"]):\n        wait_time = get_backoff_duration(attempt, base_delay=10)\n        logging.warning(f\"Backend error encountered. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Authorization errors\n    if \"401\" in error_str or \"403\" in error_str:\n        logging.error(\"Authorization error. Please check your credentials.\")\n        return False, 0\n    \n    # Invalid request errors\n    if \"400\" in error_str:\n        logging.error(\"Invalid request error. Please check your input data.\")\n        return False, 0\n    \n    # Default case - retry with standard backoff\n    wait_time = get_backoff_duration(attempt)\n    logging.warning(f\"Unexpected error. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n    return True, wait_time\n\ndef retry_with_backoff(func, max_retries=5, initial_delay=5):\n    \"\"\"\n    Retry a function with exponential backoff and improved error handling.\n    \"\"\"\n    last_error = None\n    \n    for attempt in range(max_retries):\n        try:\n            return func()\n        except gspread.exceptions.APIError as e:\n            should_retry, wait_time = handle_google_api_error(e, attempt, max_retries)\n            if not should_retry or attempt == max_retries - 1:\n                raise\n            time.sleep(wait_time)\n            last_error = e\n        except gspread.exceptions.SpreadsheetNotFound:\n            logging.error(\"Spreadsheet not found. Please check the spreadsheet ID or name.\")\n            raise\n        except gspread.exceptions.WorksheetNotFound:\n            logging.error(\"Worksheet not found. Please check the worksheet name.\")\n            raise\n        except Exception as e:\n            logging.error(f\"Unexpected error: {str(e)}\")\n            raise\n    \n    if last_error:\n        raise last_error\n    return None\n\ndef sanitize_sheet_name(title, max_length=100):\n    \"\"\"\n    Sanitize a title to be used as a Google Sheets worksheet name.\n    - Removes invalid characters\n    - Truncates to max_length\n    - Ensures uniqueness by adding a counter if needed\n    \"\"\"\n    # Remove invalid characters and replace spaces with underscores\n    sanitized = re.sub(r'[\\[\\]\\\\*?/:]', '', title)\n    sanitized = sanitized.replace(' ', '_')\n    \n    # Truncate to max_length\n    if len(sanitized) > max_length:\n        # Keep the first part of the title and add a hash of the full title\n        hash_suffix = str(hash(title))[-8:]\n        sanitized = sanitized[:max_length-9] + '_' + hash_suffix\n    \n    return sanitized\n\ndef update_sheet_with_retry(sheet, data, range_name=\"A1\"):\n    \"\"\"\n    Update a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def update():\n        try:\n            # First clear the entire sheet\n            sheet.clear()\n            time.sleep(1)  # Add a small delay after clearing\n            \n            # Validate data before updating\n            if not data or not isinstance(data, (list, tuple)):\n                raise ValueError(\"Invalid data format. Expected non-empty list or tuple.\")\n            \n            # Then update with new data\n            sheet.update(range_name=range_name, values=data, value_input_option=\"USER_ENTERED\")\n            logging.info(f\"Successfully updated sheet with {len(data)} rows of data\")\n            time.sleep(2)  # Add delay between operations\n        except gspread.exceptions.APIError as e:\n            logging.error(f\"API error during sheet update: {str(e)}\")\n            raise\n        except Exception as e:\n            logging.error(f\"Error during sheet update: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(update)\n\ndef format_sheet_with_retry(sheet, range_name, format_dict):\n    \"\"\"\n    Format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def format():\n        try:\n            sheet.format(range_name, format_dict)\n            logging.info(f\"Successfully formatted range {range_name}\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during sheet formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(format)\n\ndef batch_format_with_retry(sheet, format_requests):\n    \"\"\"\n    Batch format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def batch_format():\n        try:\n            sheet.batch_format(format_requests)\n            logging.info(f\"Successfully applied {len(format_requests)} format requests\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during batch formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(batch_format)\n\ndef validate_pr_data(pr):\n    \"\"\"\n    Validate PR data structure and required fields.\n    Returns (is_valid, error_message)\n    \"\"\"\n    required_fields = {\n        \"title\": str,\n        \"repository\": str,\n        \"number\": (int, str),  # Allow both int and str\n        \"state\": str,\n        \"createdAt\": str,\n        \"updatedAt\": str,\n        \"checkStatus\": (str, type(None))  # Allow string or None\n    }\n    \n    for field, field_type in required_fields.items():\n        if field not in pr:\n            return False, f\"Missing required field: {field}\"\n        if not isinstance(pr[field], field_type):\n            if isinstance(field_type, tuple):\n                if not any(isinstance(pr[field], t) for t in field_type):\n                    return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n            else:\n                return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n    \n    # Validate state values\n    if pr[\"state\"] not in [\"OPEN\", \"CLOSED\", \"MERGED\"]:\n        return False, f\"Invalid state value: {pr['state']}\"\n    \n    # Validate date formats\n    try:\n        datetime.fromisoformat(pr[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        datetime.fromisoformat(pr[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n    except ValueError as e:\n        return False, f\"Invalid date format: {e}\"\n    \n    return True, None\n\ndef group_prs_by_title(prs):\n    \"\"\"\n    Group PRs by title and calculate statistics.\n    \"\"\"\n    title_groups = {}\n    for pr in prs:\n        title = pr[\"title\"]\n        if title not in title_groups:\n            title_groups[title] = {\n                \"title\": title,\n                \"prs\": [],\n                \"open\": 0,\n                \"closed\": 0,\n                \"merged\": 0\n            }\n        \n        group = title_groups[title]\n        group[\"prs\"].append(pr)\n        if pr[\"state\"] == \"OPEN\":\n            group[\"open\"] += 1\n        elif pr[\"state\"] == \"CLOSED\":\n            group[\"closed\"] += 1\n        elif pr[\"state\"] == \"MERGED\":\n            group[\"merged\"] += 1\n    \n    return list(title_groups.values())\n\ndef process_consolidated_data(consolidated_file):\n    \"\"\"\n    Process consolidated PR data and validate it.\n    Returns (grouped_prs, failing_prs, errors)\n    \"\"\"\n    errors = []\n    valid_prs = []\n    \n    try:\n        with open(consolidated_file) as f:\n            prs = json.load(f)\n    except FileNotFoundError:\n        return None, None, [f\"File {consolidated_file} not found.\"]\n    except json.JSONDecodeError:\n        return None, None, [f\"Error decoding {consolidated_file}.\"]\n    \n    if not isinstance(prs, list):\n        return None, None, [\"Consolidated data must be a list of PRs.\"]\n    \n    # Validate each PR\n    for i, pr in enumerate(prs):\n        is_valid, error = validate_pr_data(pr)\n        if is_valid:\n            valid_prs.append(pr)\n        else:\n            errors.append(f\"PR at index {i}: {error}\")\n    \n    if not valid_prs:\n        return None, None, [\"No valid PRs found in consolidated data.\"]\n    \n    # Group valid PRs by title\n    grouped_prs = group_prs_by_title(valid_prs)\n    \n    # Extract failing PRs\n    failing_prs = [\n        {\n            \"title\": pr[\"title\"],\n            \"url\": f\"https://github.com/{pr['repository']}/pull/{pr['number']}\",\n            \"status\": pr[\"checkStatus\"]\n        }\n        for pr in valid_prs\n        if pr[\"state\"] == \"OPEN\" and pr[\"checkStatus\"] in [\"ERROR\", \"FAILURE\"]\n    ]\n    \n    return grouped_prs, failing_prs, errors\n\n# Main execution\nif len(sys.argv) != 3:\n    print(\"Usage: python3 upload_to_sheets.py <consolidated-prs-json-file> <failing-prs-error-state>\")\n    sys.exit(1)\n\nCONSOLIDATED_FILE = sys.argv[1]\nFAILING_PRS_ERROR = sys.argv[2].lower() == 'true'\n\n# Process consolidated data\ngrouped_prs, failing_prs, errors = process_consolidated_data(CONSOLIDATED_FILE)\n\nif errors:\n    for error in errors:\n        logging.error(error)\n    if not grouped_prs:  # Fatal errors\n        sys.exit(1)\n    else:  # Non-fatal errors\n        logging.warning(\"Some PRs were invalid but processing will continue.\")\n\n# Define the scope\nscope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n\n# Add your service account credentials\n# Check for environment variable first, then fall back to local file\ncredentials_file = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', 'concise-complex-344219-062a255ca56f.json')\ntry:\n    creds = Credentials.from_service_account_file(credentials_file, scopes=scope)\n    logging.info(f\"Using credentials from: {credentials_file}\")\nexcept FileNotFoundError:\n    # For GitHub Actions, try the file we created in the workflow\n    fallback_file = 'google-credentials.json'\n    logging.info(f\"Credentials file {credentials_file} not found, trying {fallback_file}\")\n    creds = Credentials.from_service_account_file(fallback_file, scopes=scope)\n\n# Authorize the client\nclient = gspread.authorize(creds)\n\n# Open the Google Sheet by name or ID\nspreadsheet = client.open(\"Jenkins PR Tracker\")  # or use client.open_by_key(\"YOUR_SHEET_ID\")\n\n# Create a summary sheet\ntry:\n    summary_sheet = spreadsheet.worksheet(\"Summary\")\n    logging.info(\"Summary sheet already exists. Updating it...\")\nexcept gspread.exceptions.WorksheetNotFound:\n    logging.info(\"Creating new Summary sheet...\")\n    summary_sheet = spreadsheet.add_worksheet(title=\"Summary\", rows=100, cols=10)\n\n# Prepare summary data\ntotal_prs = 0\nopen_prs = 0\nclosed_prs = 0\nmerged_prs = 0\nplugin_stats = {}\nearliest_date = None\nlatest_date = None\n\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    total_prs += len(prs)\n    open_prs += pr[\"open\"]\n    closed_prs += pr[\"closed\"]\n    merged_prs += pr[\"merged\"]\n\n    # Plugin-specific stats\n    plugin_stats[title] = {\n        \"total\": len(prs),\n        \"open\": pr[\"open\"],\n        \"closed\": pr[\"closed\"],\n        \"merged\": pr[\"merged\"]\n    }\n\n    # Find the earliest and latest dates\n    for p in prs:\n        created_at = datetime.fromisoformat(p[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        updated_at = datetime.fromisoformat(p[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n\n        if earliest_date is None or created_at < earliest_date:\n            earliest_date = created_at\n        if latest_date is None or updated_at > latest_date:\n            latest_date = updated_at\n\n# Calculate percentages\nopen_percentage = (open_prs / total_prs) * 100 if total_prs > 0 else 0\nclosed_percentage = (closed_prs / total_prs) * 100 if total_prs > 0 else 0\nmerged_percentage = (merged_prs / total_prs) * 100 if total_prs > 0 else 0\n\n# Prepare summary data for the sheet\nsummary_data = [\n    [\"PR Date Range\", f\"{earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\", \"\", \"\", \"\", \"\"],\n    [\"Overall PR Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Total PRs\", total_prs, \"\", \"\", \"\", \"\"],\n    [\"Open PRs\", open_prs, f\"{open_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Closed PRs\", closed_prs, f\"{closed_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Merged PRs\", merged_prs, f\"{merged_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin-Specific Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin\", \"Total PRs\", \"Open PRs\", \"Closed PRs\", \"Merged PRs\", \"Link to Sheet\"]\n]\n\n# Add plugin-specific stats and links to individual sheets\nfor plugin, stats in plugin_stats.items():\n    sheet_name = sanitize_sheet_name(plugin)\n    if not sheet_name:\n        logging.error(f\"Invalid sheet name generated for plugin '{plugin}'. Skipping sheet creation.\")\n        continue\n\n    try:\n        plugin_sheet = spreadsheet.worksheet(sheet_name)\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n        plugin_sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n    link = f'=HYPERLINK(\"#gid={plugin_sheet.id}\"; \"{plugin}\")'\n    summary_data.append([\n        plugin,\n        stats[\"total\"],\n        stats[\"open\"],\n        stats[\"closed\"],\n        stats[\"merged\"],\n        link\n    ])\n\n# Update the summary sheet\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Reorder sheets to make the Summary sheet first\nsheets = spreadsheet.worksheets()\nif sheets[0].title != \"Summary\":\n    summary_sheet_index = next((i for i, sheet in enumerate(sheets) if sheet.title == \"Summary\"), None)\n    if summary_sheet_index is not None:\n        spreadsheet.reorder_worksheets(\n            [sheets[summary_sheet_index]] + [sheet for i, sheet in enumerate(sheets) if i != summary_sheet_index])\n\n# Get the Summary sheet ID for the \"Back to Summary\" link\nsummary_sheet_id = summary_sheet.id\n\n# Format the summary sheet\nformat_sheet_with_retry(summary_sheet, \"A1:F1\", {\n    \"textFormat\": {\n        \"bold\": True\n    },\n    \"backgroundColor\": {\n        \"red\": 0.9,  # Light gray background\n        \"green\": 0.9,\n        \"blue\": 0.9,\n        \"alpha\": 1.0\n    },\n    \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n})\n\n# Create a new sheet for failing PRs\nif failing_prs and isinstance(failing_prs, list):\n    try:\n        failing_prs_sheet = spreadsheet.worksheet(\"Failing PRs\")\n        logging.info(\"Failing PRs sheet already exists. Updating it...\")\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(\"Creating new Failing PRs sheet...\")\n        failing_prs_sheet = spreadsheet.add_worksheet(title=\"Failing PRs\", rows=100, cols=10)\n\n    # Prepare the data for the failing PRs sheet\n    failing_prs_data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Title\", \"URL\", \"Status\"]\n    ]\n\n    # Process each failing PR\n    for pr in failing_prs:\n        failing_prs_data.append([\n            pr[\"title\"],\n            f'=HYPERLINK(\"{pr[\"url\"]}\"; \"{pr[\"url\"]}\")',\n            pr[\"status\"]\n        ])\n\n    # Clear the sheet and update it with the new data\n    update_sheet_with_retry(failing_prs_sheet, failing_prs_data)\n\n    # Format the column titles (bold font and background color)\n    format_sheet_with_retry(failing_prs_sheet, \"A3:C3\", {  # Format only the column titles (row 3)\n        \"textFormat\": {\n            \"bold\": True\n        },\n        \"backgroundColor\": {\n            \"red\": 0.9,  # Light gray background\n            \"green\": 0.9,\n            \"blue\": 0.9,\n            \"alpha\": 1.0\n        },\n        \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n    })\n\n    # Calculate failing PRs count\n    failing_prs_count = len(failing_prs)\nelse:\n    failing_prs_count = 0\n    logging.warning(\"No failing PRs found in the data.\")\n\n# Add a link to the \"Failing PRs\" sheet in the \"Summary\" sheet and include the count\nif failing_prs and 'failing_prs_sheet' in locals():\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", f'=HYPERLINK(\"#gid={failing_prs_sheet.id}\"; \"Failing PRs\")'])\nelse:\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", \"No failing PRs found\"])\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Iterate through each PR group and create a new sheet for each title\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    sheet_name = sanitize_sheet_name(title)\n\n    # Prepare the data for the sheet\n    data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [title, \"\", \"\", \"\", \"\"],  # Add title without label\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Repository\", \"PR Number\", \"State\", \"Created At\", \"Updated At\"]\n    ]\n    for p in prs:\n        # Add hyperlinks to the Repository and PR Number columns\n        repo_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}\"; \"{p[\"repository\"]}\")'\n        pr_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}/pull/{p[\"number\"]}\"; \"{p[\"number\"]}\")'\n        data.append([repo_link, pr_link, p[\"state\"], p[\"createdAt\"], p[\"updatedAt\"]])\n\n    try:\n        # Check if a sheet with the same title already exists\n        try:\n            sheet = spreadsheet.worksheet(sheet_name)\n            logging.info(f\"Sheet '{sheet_name}' already exists. Updating it...\")\n        except gspread.exceptions.WorksheetNotFound:\n            # Create a new sheet if it doesn't exist\n            logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n            sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n        # Update sheet with retry logic\n        update_sheet_with_retry(sheet, data)\n\n        # Format the \"Back to Summary\" row - only format the first cell\n        format_sheet_with_retry(sheet, \"A1\", {\n            \"textFormat\": {\n                \"fontSize\": 10\n            }\n        })\n\n        # Format the title row - only format the first cell\n        format_sheet_with_retry(sheet, \"A2\", {\n            \"textFormat\": {\n                \"bold\": True,\n                \"fontSize\": 10\n            },\n            \"horizontalAlignment\": \"LEFT\"\n        })\n\n        # Format the column titles\n        format_sheet_with_retry(sheet, \"A4:E4\", {\n            \"textFormat\": {\n                \"bold\": True\n            },\n            \"backgroundColor\": {\n                \"red\": 0.9,\n                \"green\": 0.9,\n                \"blue\": 0.9,\n                \"alpha\": 1.0\n            },\n            \"horizontalAlignment\": \"CENTER\"\n        })\n\n        # Apply conditional formatting based on PR state\n        format_requests = []\n        for row_idx, p in enumerate(prs, start=5):  # Start from row 5 (skip header, title, and \"Back to Summary\" rows)\n            color = {\n                \"MERGED\": {\"red\": 0.0, \"green\": 1.0, \"blue\": 0.0, \"alpha\": 1.0},\n                \"OPEN\": {\"red\": 1.0, \"green\": 0.5, \"blue\": 0.0, \"alpha\": 1.0},\n                \"CLOSED\": {\"red\": 1.0, \"green\": 0.0, \"blue\": 0.0, \"alpha\": 1.0},\n            }.get(p[\"state\"], {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0, \"alpha\": 1.0})\n\n            format_requests.append({\n                \"range\": f\"A{row_idx}:E{row_idx}\",\n                \"format\": {\n                    \"backgroundColor\": color\n                }\n            })\n\n        # Apply all formatting requests in a single batch\n        if format_requests:\n            batch_format_with_retry(sheet, format_requests)\n\n        # Add a longer delay between sheets to avoid rate limits\n        time.sleep(10)  # Increased delay between sheets\n\n    except gspread.exceptions.APIError as e:\n        logging.error(f\"Failed to update sheet '{sheet_name}': {e}\")\n        continue\n\nlogging.info(\"Data has been uploaded to Google Sheets.\")\n"
                    }
                ]
            },
            "createdAt": 1743511800968,
            "isInternal": false,
            "rawContent": {
                "type": "doc",
                "content": [
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Could we leverage the data found in \"successful_builds.csv \" within \"upload_to_sheets.py\" by creating a new sheet in the Google document?"
                            }
                        ]
                    },
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "This file lists the PRs that don't build successfully on the Jenkins infrastructure, but build correctly locally, with the JDK they built with cited."
                            }
                        ]
                    },
                    {
                        "type": "paragraph"
                    },
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "What could be a short but good name for this sheet?"
                            }
                        ]
                    },
                    {
                        "type": "paragraph"
                    },
                    {
                        "type": "paragraph",
                        "content": [
                            {
                                "type": "text",
                                "text": "Thanks."
                            }
                        ]
                    }
                ]
            }
        },
        {
            "id": "c48aa444-f900-495c-ac3f-a344717e37e2",
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "To leverage the data from \"successful_builds.csv\" within the \"upload_to_sheets.py\" script, you can create a new sheet in the Google document to store this information. Since the file lists PRs that build successfully locally but not on Jenkins, a suitable name for this sheet could be \"Local Success Builds\".\n\nHere's a brief outline of how you can modify the script to add this functionality:\n\n1. **Read the CSV File**: Use Python's `csv` module to read the \"successful_builds.csv\" file.\n\n2. **Create a New Sheet**: Add a new worksheet to the Google Sheets document with the name \"Local Success Builds\".\n\n3. **Upload Data**: Populate the new sheet with the data from the CSV file.\n\nHere's a code snippet to help you get started:\n\n```python\nimport csv\n\ndef upload_successful_builds_to_sheet(spreadsheet, csv_file_path):\n    # Read the CSV file\n    with open(csv_file_path, mode='r') as file:\n        csv_reader = csv.reader(file)\n        data = list(csv_reader)\n\n    # Sanitize the sheet name\n    sheet_name = sanitize_sheet_name(\"Local Success Builds\")\n\n    # Create or get the worksheet\n    try:\n        success_builds_sheet = spreadsheet.worksheet(sheet_name)\n        logging.info(f\"Sheet '{sheet_name}' already exists. Updating it...\")\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n        success_builds_sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n    # Update the sheet with the CSV data\n    update_sheet_with_retry(success_builds_sheet, data)\n\n# Example usage\ncsv_file_path = \"path/to/successful_builds.csv\"\nupload_successful_builds_to_sheet(spreadsheet, csv_file_path)\n```\n\nMake sure to replace `\"path/to/successful_builds.csv\"` with the actual path to your CSV file. This function reads the CSV file, creates a new sheet (or updates it if it already exists), and uploads the data to the sheet."
                }
            ],
            "context": {
                "currentFile": {
                    "path": "upload_to_sheets.py",
                    "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                    "sources": [
                        "opened_tab"
                    ],
                    "content": "import gspread\nimport gspread.exceptions  # Ensure exceptions are properly referenced if not already imported\nfrom google.oauth2.service_account import Credentials\nimport json\nimport time\nimport logging\nfrom datetime import datetime\nimport sys\nimport re\nfrom time import sleep\nimport random\nimport os\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogging.info(\"Starting script...\")\n\ndef get_backoff_duration(attempt, base_delay=5, max_delay=300):\n    \"\"\"\n    Calculate exponential backoff duration with jitter.\n    \n    Args:\n        attempt: The current retry attempt number (0-based)\n        base_delay: The base delay in seconds (default: 5)\n        max_delay: Maximum delay in seconds (default: 300)\n    \n    Returns:\n        Delay duration in seconds\n    \"\"\"\n    delay = min(base_delay * (2 ** attempt), max_delay)\n    jitter = random.uniform(0, 0.1 * delay)  # Add up to 10% jitter\n    return delay + jitter\n\ndef handle_google_api_error(e, attempt, max_retries):\n    \"\"\"\n    Handle different types of Google Sheets API errors.\n    \n    Returns:\n        tuple: (should_retry, wait_time)\n    \"\"\"\n    error_str = str(e).lower()\n    \n    # Rate limit errors\n    if \"429\" in error_str or \"quota\" in error_str:\n        wait_time = get_backoff_duration(attempt)\n        logging.warning(f\"Rate limit exceeded. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Backend errors (500s)\n    if any(code in error_str for code in [\"500\", \"502\", \"503\", \"504\"]):\n        wait_time = get_backoff_duration(attempt, base_delay=10)\n        logging.warning(f\"Backend error encountered. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Authorization errors\n    if \"401\" in error_str or \"403\" in error_str:\n        logging.error(\"Authorization error. Please check your credentials.\")\n        return False, 0\n    \n    # Invalid request errors\n    if \"400\" in error_str:\n        logging.error(\"Invalid request error. Please check your input data.\")\n        return False, 0\n    \n    # Default case - retry with standard backoff\n    wait_time = get_backoff_duration(attempt)\n    logging.warning(f\"Unexpected error. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n    return True, wait_time\n\ndef retry_with_backoff(func, max_retries=5, initial_delay=5):\n    \"\"\"\n    Retry a function with exponential backoff and improved error handling.\n    \"\"\"\n    last_error = None\n    \n    for attempt in range(max_retries):\n        try:\n            return func()\n        except gspread.exceptions.APIError as e:\n            should_retry, wait_time = handle_google_api_error(e, attempt, max_retries)\n            if not should_retry or attempt == max_retries - 1:\n                raise\n            time.sleep(wait_time)\n            last_error = e\n        except gspread.exceptions.SpreadsheetNotFound:\n            logging.error(\"Spreadsheet not found. Please check the spreadsheet ID or name.\")\n            raise\n        except gspread.exceptions.WorksheetNotFound:\n            logging.error(\"Worksheet not found. Please check the worksheet name.\")\n            raise\n        except Exception as e:\n            logging.error(f\"Unexpected error: {str(e)}\")\n            raise\n    \n    if last_error:\n        raise last_error\n    return None\n\ndef sanitize_sheet_name(title, max_length=100):\n    \"\"\"\n    Sanitize a title to be used as a Google Sheets worksheet name.\n    - Removes invalid characters\n    - Truncates to max_length\n    - Ensures uniqueness by adding a counter if needed\n    \"\"\"\n    # Remove invalid characters and replace spaces with underscores\n    sanitized = re.sub(r'[\\[\\]\\\\*?/:]', '', title)\n    sanitized = sanitized.replace(' ', '_')\n    \n    # Truncate to max_length\n    if len(sanitized) > max_length:\n        # Keep the first part of the title and add a hash of the full title\n        hash_suffix = str(hash(title))[-8:]\n        sanitized = sanitized[:max_length-9] + '_' + hash_suffix\n    \n    return sanitized\n\ndef update_sheet_with_retry(sheet, data, range_name=\"A1\"):\n    \"\"\"\n    Update a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def update():\n        try:\n            # First clear the entire sheet\n            sheet.clear()\n            time.sleep(1)  # Add a small delay after clearing\n            \n            # Validate data before updating\n            if not data or not isinstance(data, (list, tuple)):\n                raise ValueError(\"Invalid data format. Expected non-empty list or tuple.\")\n            \n            # Then update with new data\n            sheet.update(range_name=range_name, values=data, value_input_option=\"USER_ENTERED\")\n            logging.info(f\"Successfully updated sheet with {len(data)} rows of data\")\n            time.sleep(2)  # Add delay between operations\n        except gspread.exceptions.APIError as e:\n            logging.error(f\"API error during sheet update: {str(e)}\")\n            raise\n        except Exception as e:\n            logging.error(f\"Error during sheet update: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(update)\n\ndef format_sheet_with_retry(sheet, range_name, format_dict):\n    \"\"\"\n    Format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def format():\n        try:\n            sheet.format(range_name, format_dict)\n            logging.info(f\"Successfully formatted range {range_name}\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during sheet formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(format)\n\ndef batch_format_with_retry(sheet, format_requests):\n    \"\"\"\n    Batch format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def batch_format():\n        try:\n            sheet.batch_format(format_requests)\n            logging.info(f\"Successfully applied {len(format_requests)} format requests\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during batch formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(batch_format)\n\ndef validate_pr_data(pr):\n    \"\"\"\n    Validate PR data structure and required fields.\n    Returns (is_valid, error_message)\n    \"\"\"\n    required_fields = {\n        \"title\": str,\n        \"repository\": str,\n        \"number\": (int, str),  # Allow both int and str\n        \"state\": str,\n        \"createdAt\": str,\n        \"updatedAt\": str,\n        \"checkStatus\": (str, type(None))  # Allow string or None\n    }\n    \n    for field, field_type in required_fields.items():\n        if field not in pr:\n            return False, f\"Missing required field: {field}\"\n        if not isinstance(pr[field], field_type):\n            if isinstance(field_type, tuple):\n                if not any(isinstance(pr[field], t) for t in field_type):\n                    return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n            else:\n                return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n    \n    # Validate state values\n    if pr[\"state\"] not in [\"OPEN\", \"CLOSED\", \"MERGED\"]:\n        return False, f\"Invalid state value: {pr['state']}\"\n    \n    # Validate date formats\n    try:\n        datetime.fromisoformat(pr[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        datetime.fromisoformat(pr[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n    except ValueError as e:\n        return False, f\"Invalid date format: {e}\"\n    \n    return True, None\n\ndef group_prs_by_title(prs):\n    \"\"\"\n    Group PRs by title and calculate statistics.\n    \"\"\"\n    title_groups = {}\n    for pr in prs:\n        title = pr[\"title\"]\n        if title not in title_groups:\n            title_groups[title] = {\n                \"title\": title,\n                \"prs\": [],\n                \"open\": 0,\n                \"closed\": 0,\n                \"merged\": 0\n            }\n        \n        group = title_groups[title]\n        group[\"prs\"].append(pr)\n        if pr[\"state\"] == \"OPEN\":\n            group[\"open\"] += 1\n        elif pr[\"state\"] == \"CLOSED\":\n            group[\"closed\"] += 1\n        elif pr[\"state\"] == \"MERGED\":\n            group[\"merged\"] += 1\n    \n    return list(title_groups.values())\n\ndef process_consolidated_data(consolidated_file):\n    \"\"\"\n    Process consolidated PR data and validate it.\n    Returns (grouped_prs, failing_prs, errors)\n    \"\"\"\n    errors = []\n    valid_prs = []\n    \n    try:\n        with open(consolidated_file) as f:\n            prs = json.load(f)\n    except FileNotFoundError:\n        return None, None, [f\"File {consolidated_file} not found.\"]\n    except json.JSONDecodeError:\n        return None, None, [f\"Error decoding {consolidated_file}.\"]\n    \n    if not isinstance(prs, list):\n        return None, None, [\"Consolidated data must be a list of PRs.\"]\n    \n    # Validate each PR\n    for i, pr in enumerate(prs):\n        is_valid, error = validate_pr_data(pr)\n        if is_valid:\n            valid_prs.append(pr)\n        else:\n            errors.append(f\"PR at index {i}: {error}\")\n    \n    if not valid_prs:\n        return None, None, [\"No valid PRs found in consolidated data.\"]\n    \n    # Group valid PRs by title\n    grouped_prs = group_prs_by_title(valid_prs)\n    \n    # Extract failing PRs\n    failing_prs = [\n        {\n            \"title\": pr[\"title\"],\n            \"url\": f\"https://github.com/{pr['repository']}/pull/{pr['number']}\",\n            \"status\": pr[\"checkStatus\"]\n        }\n        for pr in valid_prs\n        if pr[\"state\"] == \"OPEN\" and pr[\"checkStatus\"] in [\"ERROR\", \"FAILURE\"]\n    ]\n    \n    return grouped_prs, failing_prs, errors\n\n# Main execution\nif len(sys.argv) != 3:\n    print(\"Usage: python3 upload_to_sheets.py <consolidated-prs-json-file> <failing-prs-error-state>\")\n    sys.exit(1)\n\nCONSOLIDATED_FILE = sys.argv[1]\nFAILING_PRS_ERROR = sys.argv[2].lower() == 'true'\n\n# Process consolidated data\ngrouped_prs, failing_prs, errors = process_consolidated_data(CONSOLIDATED_FILE)\n\nif errors:\n    for error in errors:\n        logging.error(error)\n    if not grouped_prs:  # Fatal errors\n        sys.exit(1)\n    else:  # Non-fatal errors\n        logging.warning(\"Some PRs were invalid but processing will continue.\")\n\n# Define the scope\nscope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n\n# Add your service account credentials\n# Check for environment variable first, then fall back to local file\ncredentials_file = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', 'concise-complex-344219-062a255ca56f.json')\ntry:\n    creds = Credentials.from_service_account_file(credentials_file, scopes=scope)\n    logging.info(f\"Using credentials from: {credentials_file}\")\nexcept FileNotFoundError:\n    # For GitHub Actions, try the file we created in the workflow\n    fallback_file = 'google-credentials.json'\n    logging.info(f\"Credentials file {credentials_file} not found, trying {fallback_file}\")\n    creds = Credentials.from_service_account_file(fallback_file, scopes=scope)\n\n# Authorize the client\nclient = gspread.authorize(creds)\n\n# Open the Google Sheet by name or ID\nspreadsheet = client.open(\"Jenkins PR Tracker\")  # or use client.open_by_key(\"YOUR_SHEET_ID\")\n\n# Create a summary sheet\ntry:\n    summary_sheet = spreadsheet.worksheet(\"Summary\")\n    logging.info(\"Summary sheet already exists. Updating it...\")\nexcept gspread.exceptions.WorksheetNotFound:\n    logging.info(\"Creating new Summary sheet...\")\n    summary_sheet = spreadsheet.add_worksheet(title=\"Summary\", rows=100, cols=10)\n\n# Prepare summary data\ntotal_prs = 0\nopen_prs = 0\nclosed_prs = 0\nmerged_prs = 0\nplugin_stats = {}\nearliest_date = None\nlatest_date = None\n\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    total_prs += len(prs)\n    open_prs += pr[\"open\"]\n    closed_prs += pr[\"closed\"]\n    merged_prs += pr[\"merged\"]\n\n    # Plugin-specific stats\n    plugin_stats[title] = {\n        \"total\": len(prs),\n        \"open\": pr[\"open\"],\n        \"closed\": pr[\"closed\"],\n        \"merged\": pr[\"merged\"]\n    }\n\n    # Find the earliest and latest dates\n    for p in prs:\n        created_at = datetime.fromisoformat(p[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        updated_at = datetime.fromisoformat(p[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n\n        if earliest_date is None or created_at < earliest_date:\n            earliest_date = created_at\n        if latest_date is None or updated_at > latest_date:\n            latest_date = updated_at\n\n# Calculate percentages\nopen_percentage = (open_prs / total_prs) * 100 if total_prs > 0 else 0\nclosed_percentage = (closed_prs / total_prs) * 100 if total_prs > 0 else 0\nmerged_percentage = (merged_prs / total_prs) * 100 if total_prs > 0 else 0\n\n# Prepare summary data for the sheet\nsummary_data = [\n    [\"PR Date Range\", f\"{earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\", \"\", \"\", \"\", \"\"],\n    [\"Overall PR Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Total PRs\", total_prs, \"\", \"\", \"\", \"\"],\n    [\"Open PRs\", open_prs, f\"{open_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Closed PRs\", closed_prs, f\"{closed_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Merged PRs\", merged_prs, f\"{merged_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin-Specific Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin\", \"Total PRs\", \"Open PRs\", \"Closed PRs\", \"Merged PRs\", \"Link to Sheet\"]\n]\n\n# Add plugin-specific stats and links to individual sheets\nfor plugin, stats in plugin_stats.items():\n    sheet_name = sanitize_sheet_name(plugin)\n    if not sheet_name:\n        logging.error(f\"Invalid sheet name generated for plugin '{plugin}'. Skipping sheet creation.\")\n        continue\n\n    try:\n        plugin_sheet = spreadsheet.worksheet(sheet_name)\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n        plugin_sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n    link = f'=HYPERLINK(\"#gid={plugin_sheet.id}\"; \"{plugin}\")'\n    summary_data.append([\n        plugin,\n        stats[\"total\"],\n        stats[\"open\"],\n        stats[\"closed\"],\n        stats[\"merged\"],\n        link\n    ])\n\n# Update the summary sheet\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Reorder sheets to make the Summary sheet first\nsheets = spreadsheet.worksheets()\nif sheets[0].title != \"Summary\":\n    summary_sheet_index = next((i for i, sheet in enumerate(sheets) if sheet.title == \"Summary\"), None)\n    if summary_sheet_index is not None:\n        spreadsheet.reorder_worksheets(\n            [sheets[summary_sheet_index]] + [sheet for i, sheet in enumerate(sheets) if i != summary_sheet_index])\n\n# Get the Summary sheet ID for the \"Back to Summary\" link\nsummary_sheet_id = summary_sheet.id\n\n# Format the summary sheet\nformat_sheet_with_retry(summary_sheet, \"A1:F1\", {\n    \"textFormat\": {\n        \"bold\": True\n    },\n    \"backgroundColor\": {\n        \"red\": 0.9,  # Light gray background\n        \"green\": 0.9,\n        \"blue\": 0.9,\n        \"alpha\": 1.0\n    },\n    \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n})\n\n# Create a new sheet for failing PRs\nif failing_prs and isinstance(failing_prs, list):\n    try:\n        failing_prs_sheet = spreadsheet.worksheet(\"Failing PRs\")\n        logging.info(\"Failing PRs sheet already exists. Updating it...\")\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(\"Creating new Failing PRs sheet...\")\n        failing_prs_sheet = spreadsheet.add_worksheet(title=\"Failing PRs\", rows=100, cols=10)\n\n    # Prepare the data for the failing PRs sheet\n    failing_prs_data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Title\", \"URL\", \"Status\"]\n    ]\n\n    # Process each failing PR\n    for pr in failing_prs:\n        failing_prs_data.append([\n            pr[\"title\"],\n            f'=HYPERLINK(\"{pr[\"url\"]}\"; \"{pr[\"url\"]}\")',\n            pr[\"status\"]\n        ])\n\n    # Clear the sheet and update it with the new data\n    update_sheet_with_retry(failing_prs_sheet, failing_prs_data)\n\n    # Format the column titles (bold font and background color)\n    format_sheet_with_retry(failing_prs_sheet, \"A3:C3\", {  # Format only the column titles (row 3)\n        \"textFormat\": {\n            \"bold\": True\n        },\n        \"backgroundColor\": {\n            \"red\": 0.9,  # Light gray background\n            \"green\": 0.9,\n            \"blue\": 0.9,\n            \"alpha\": 1.0\n        },\n        \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n    })\n\n    # Calculate failing PRs count\n    failing_prs_count = len(failing_prs)\nelse:\n    failing_prs_count = 0\n    logging.warning(\"No failing PRs found in the data.\")\n\n# Add a link to the \"Failing PRs\" sheet in the \"Summary\" sheet and include the count\nif failing_prs and 'failing_prs_sheet' in locals():\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", f'=HYPERLINK(\"#gid={failing_prs_sheet.id}\"; \"Failing PRs\")'])\nelse:\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", \"No failing PRs found\"])\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Iterate through each PR group and create a new sheet for each title\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    sheet_name = sanitize_sheet_name(title)\n\n    # Prepare the data for the sheet\n    data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [title, \"\", \"\", \"\", \"\"],  # Add title without label\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Repository\", \"PR Number\", \"State\", \"Created At\", \"Updated At\"]\n    ]\n    for p in prs:\n        # Add hyperlinks to the Repository and PR Number columns\n        repo_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}\"; \"{p[\"repository\"]}\")'\n        pr_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}/pull/{p[\"number\"]}\"; \"{p[\"number\"]}\")'\n        data.append([repo_link, pr_link, p[\"state\"], p[\"createdAt\"], p[\"updatedAt\"]])\n\n    try:\n        # Check if a sheet with the same title already exists\n        try:\n            sheet = spreadsheet.worksheet(sheet_name)\n            logging.info(f\"Sheet '{sheet_name}' already exists. Updating it...\")\n        except gspread.exceptions.WorksheetNotFound:\n            # Create a new sheet if it doesn't exist\n            logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n            sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n        # Update sheet with retry logic\n        update_sheet_with_retry(sheet, data)\n\n        # Format the \"Back to Summary\" row - only format the first cell\n        format_sheet_with_retry(sheet, \"A1\", {\n            \"textFormat\": {\n                \"fontSize\": 10\n            }\n        })\n\n        # Format the title row - only format the first cell\n        format_sheet_with_retry(sheet, \"A2\", {\n            \"textFormat\": {\n                \"bold\": True,\n                \"fontSize\": 10\n            },\n            \"horizontalAlignment\": \"LEFT\"\n        })\n\n        # Format the column titles\n        format_sheet_with_retry(sheet, \"A4:E4\", {\n            \"textFormat\": {\n                \"bold\": True\n            },\n            \"backgroundColor\": {\n                \"red\": 0.9,\n                \"green\": 0.9,\n                \"blue\": 0.9,\n                \"alpha\": 1.0\n            },\n            \"horizontalAlignment\": \"CENTER\"\n        })\n\n        # Apply conditional formatting based on PR state\n        format_requests = []\n        for row_idx, p in enumerate(prs, start=5):  # Start from row 5 (skip header, title, and \"Back to Summary\" rows)\n            color = {\n                \"MERGED\": {\"red\": 0.0, \"green\": 1.0, \"blue\": 0.0, \"alpha\": 1.0},\n                \"OPEN\": {\"red\": 1.0, \"green\": 0.5, \"blue\": 0.0, \"alpha\": 1.0},\n                \"CLOSED\": {\"red\": 1.0, \"green\": 0.0, \"blue\": 0.0, \"alpha\": 1.0},\n            }.get(p[\"state\"], {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0, \"alpha\": 1.0})\n\n            format_requests.append({\n                \"range\": f\"A{row_idx}:E{row_idx}\",\n                \"format\": {\n                    \"backgroundColor\": color\n                }\n            })\n\n        # Apply all formatting requests in a single batch\n        if format_requests:\n            batch_format_with_retry(sheet, format_requests)\n\n        # Add a longer delay between sheets to avoid rate limits\n        time.sleep(10)  # Increased delay between sheets\n\n    except gspread.exceptions.APIError as e:\n        logging.error(f\"Failed to update sheet '{sheet_name}': {e}\")\n        continue\n\nlogging.info(\"Data has been uploaded to Google Sheets.\")\n"
                },
                "codebaseEnabled": true,
                "ragFiles": [
                    {
                        "path": "Dockerfile",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\Dockerfile",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/spreadsheet.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\spreadsheet.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/worksheet.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\worksheet.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/pip/_vendor/urllib3/contrib/appengine.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\pip\\_vendor\\urllib3\\contrib\\appengine.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_credentials_base.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_credentials_base.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_helpers.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_helpers.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_credentials_async.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_credentials_async.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/metrics.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\metrics.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_exponential_backoff.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_exponential_backoff.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/_refresh_worker.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\_refresh_worker.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/aio/__init__.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\aio\\__init__.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/crypt/base.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\crypt\\base.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/transport/_custom_tls_signer.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\transport\\_custom_tls_signer.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/auth/transport/_mtls_helper.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\auth\\transport\\_mtls_helper.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/webauthn_handler_factory.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\webauthn_handler_factory.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/utils.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\utils.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/webauthn_types.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\webauthn_types.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/google/oauth2/sts.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\google\\oauth2\\sts.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "venv/lib/python3.13/site-packages/gspread/client.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\venv\\lib\\python3.13\\site-packages\\gspread\\client.py",
                        "sources": [
                            "keyword_search"
                        ]
                    },
                    {
                        "path": "test-pr-builds.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\test-pr-builds.sh",
                        "sources": []
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": []
                    },
                    {
                        "path": "group-prs.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\group-prs.sh",
                        "sources": []
                    },
                    {
                        "path": "update-daily.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\update-daily.sh",
                        "sources": []
                    },
                    {
                        "path": "generate-report.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\generate-report.sh",
                        "sources": []
                    },
                    {
                        "path": "analyze-junit5-prs.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\analyze-junit5-prs.sh",
                        "sources": []
                    },
                    {
                        "path": "data\\consolidated\\test_results.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\test_results.csv",
                        "sources": []
                    },
                    {
                        "path": "data\\consolidated\\successful_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\successful_builds.csv",
                        "sources": []
                    },
                    {
                        "path": "data/consolidated/failed_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\failed_builds.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "data/consolidated/successful_builds.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\successful_builds.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "data/consolidated/test_results.csv",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\data\\consolidated\\test_results.csv",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "update-daily.sh",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\update-daily.sh",
                        "sources": [
                            "opened_tab"
                        ]
                    },
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "opened_tab"
                        ]
                    }
                ],
                "rerankedFiles": [],
                "usedFilePaths": [
                    {
                        "path": "upload_to_sheets.py",
                        "fsPath": "C:\\support\\users\\dev\\jenkins\\alpha-omega\\alpha-omega-stats\\upload_to_sheets.py",
                        "sources": [
                            "opened_tab"
                        ],
                        "content": "import gspread\nimport gspread.exceptions  # Ensure exceptions are properly referenced if not already imported\nfrom google.oauth2.service_account import Credentials\nimport json\nimport time\nimport logging\nfrom datetime import datetime\nimport sys\nimport re\nfrom time import sleep\nimport random\nimport os\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogging.info(\"Starting script...\")\n\ndef get_backoff_duration(attempt, base_delay=5, max_delay=300):\n    \"\"\"\n    Calculate exponential backoff duration with jitter.\n    \n    Args:\n        attempt: The current retry attempt number (0-based)\n        base_delay: The base delay in seconds (default: 5)\n        max_delay: Maximum delay in seconds (default: 300)\n    \n    Returns:\n        Delay duration in seconds\n    \"\"\"\n    delay = min(base_delay * (2 ** attempt), max_delay)\n    jitter = random.uniform(0, 0.1 * delay)  # Add up to 10% jitter\n    return delay + jitter\n\ndef handle_google_api_error(e, attempt, max_retries):\n    \"\"\"\n    Handle different types of Google Sheets API errors.\n    \n    Returns:\n        tuple: (should_retry, wait_time)\n    \"\"\"\n    error_str = str(e).lower()\n    \n    # Rate limit errors\n    if \"429\" in error_str or \"quota\" in error_str:\n        wait_time = get_backoff_duration(attempt)\n        logging.warning(f\"Rate limit exceeded. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Backend errors (500s)\n    if any(code in error_str for code in [\"500\", \"502\", \"503\", \"504\"]):\n        wait_time = get_backoff_duration(attempt, base_delay=10)\n        logging.warning(f\"Backend error encountered. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n        return True, wait_time\n    \n    # Authorization errors\n    if \"401\" in error_str or \"403\" in error_str:\n        logging.error(\"Authorization error. Please check your credentials.\")\n        return False, 0\n    \n    # Invalid request errors\n    if \"400\" in error_str:\n        logging.error(\"Invalid request error. Please check your input data.\")\n        return False, 0\n    \n    # Default case - retry with standard backoff\n    wait_time = get_backoff_duration(attempt)\n    logging.warning(f\"Unexpected error. Attempt {attempt + 1}/{max_retries}. Waiting {wait_time:.1f} seconds...\")\n    return True, wait_time\n\ndef retry_with_backoff(func, max_retries=5, initial_delay=5):\n    \"\"\"\n    Retry a function with exponential backoff and improved error handling.\n    \"\"\"\n    last_error = None\n    \n    for attempt in range(max_retries):\n        try:\n            return func()\n        except gspread.exceptions.APIError as e:\n            should_retry, wait_time = handle_google_api_error(e, attempt, max_retries)\n            if not should_retry or attempt == max_retries - 1:\n                raise\n            time.sleep(wait_time)\n            last_error = e\n        except gspread.exceptions.SpreadsheetNotFound:\n            logging.error(\"Spreadsheet not found. Please check the spreadsheet ID or name.\")\n            raise\n        except gspread.exceptions.WorksheetNotFound:\n            logging.error(\"Worksheet not found. Please check the worksheet name.\")\n            raise\n        except Exception as e:\n            logging.error(f\"Unexpected error: {str(e)}\")\n            raise\n    \n    if last_error:\n        raise last_error\n    return None\n\ndef sanitize_sheet_name(title, max_length=100):\n    \"\"\"\n    Sanitize a title to be used as a Google Sheets worksheet name.\n    - Removes invalid characters\n    - Truncates to max_length\n    - Ensures uniqueness by adding a counter if needed\n    \"\"\"\n    # Remove invalid characters and replace spaces with underscores\n    sanitized = re.sub(r'[\\[\\]\\\\*?/:]', '', title)\n    sanitized = sanitized.replace(' ', '_')\n    \n    # Truncate to max_length\n    if len(sanitized) > max_length:\n        # Keep the first part of the title and add a hash of the full title\n        hash_suffix = str(hash(title))[-8:]\n        sanitized = sanitized[:max_length-9] + '_' + hash_suffix\n    \n    return sanitized\n\ndef update_sheet_with_retry(sheet, data, range_name=\"A1\"):\n    \"\"\"\n    Update a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def update():\n        try:\n            # First clear the entire sheet\n            sheet.clear()\n            time.sleep(1)  # Add a small delay after clearing\n            \n            # Validate data before updating\n            if not data or not isinstance(data, (list, tuple)):\n                raise ValueError(\"Invalid data format. Expected non-empty list or tuple.\")\n            \n            # Then update with new data\n            sheet.update(range_name=range_name, values=data, value_input_option=\"USER_ENTERED\")\n            logging.info(f\"Successfully updated sheet with {len(data)} rows of data\")\n            time.sleep(2)  # Add delay between operations\n        except gspread.exceptions.APIError as e:\n            logging.error(f\"API error during sheet update: {str(e)}\")\n            raise\n        except Exception as e:\n            logging.error(f\"Error during sheet update: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(update)\n\ndef format_sheet_with_retry(sheet, range_name, format_dict):\n    \"\"\"\n    Format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def format():\n        try:\n            sheet.format(range_name, format_dict)\n            logging.info(f\"Successfully formatted range {range_name}\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during sheet formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(format)\n\ndef batch_format_with_retry(sheet, format_requests):\n    \"\"\"\n    Batch format a sheet with enhanced retry logic and rate limiting.\n    \"\"\"\n    def batch_format():\n        try:\n            sheet.batch_format(format_requests)\n            logging.info(f\"Successfully applied {len(format_requests)} format requests\")\n            time.sleep(2)  # Add delay between operations\n        except Exception as e:\n            logging.error(f\"Error during batch formatting: {str(e)}\")\n            raise\n    \n    return retry_with_backoff(batch_format)\n\ndef validate_pr_data(pr):\n    \"\"\"\n    Validate PR data structure and required fields.\n    Returns (is_valid, error_message)\n    \"\"\"\n    required_fields = {\n        \"title\": str,\n        \"repository\": str,\n        \"number\": (int, str),  # Allow both int and str\n        \"state\": str,\n        \"createdAt\": str,\n        \"updatedAt\": str,\n        \"checkStatus\": (str, type(None))  # Allow string or None\n    }\n    \n    for field, field_type in required_fields.items():\n        if field not in pr:\n            return False, f\"Missing required field: {field}\"\n        if not isinstance(pr[field], field_type):\n            if isinstance(field_type, tuple):\n                if not any(isinstance(pr[field], t) for t in field_type):\n                    return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n            else:\n                return False, f\"Invalid type for {field}: expected {field_type}, got {type(pr[field])}\"\n    \n    # Validate state values\n    if pr[\"state\"] not in [\"OPEN\", \"CLOSED\", \"MERGED\"]:\n        return False, f\"Invalid state value: {pr['state']}\"\n    \n    # Validate date formats\n    try:\n        datetime.fromisoformat(pr[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        datetime.fromisoformat(pr[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n    except ValueError as e:\n        return False, f\"Invalid date format: {e}\"\n    \n    return True, None\n\ndef group_prs_by_title(prs):\n    \"\"\"\n    Group PRs by title and calculate statistics.\n    \"\"\"\n    title_groups = {}\n    for pr in prs:\n        title = pr[\"title\"]\n        if title not in title_groups:\n            title_groups[title] = {\n                \"title\": title,\n                \"prs\": [],\n                \"open\": 0,\n                \"closed\": 0,\n                \"merged\": 0\n            }\n        \n        group = title_groups[title]\n        group[\"prs\"].append(pr)\n        if pr[\"state\"] == \"OPEN\":\n            group[\"open\"] += 1\n        elif pr[\"state\"] == \"CLOSED\":\n            group[\"closed\"] += 1\n        elif pr[\"state\"] == \"MERGED\":\n            group[\"merged\"] += 1\n    \n    return list(title_groups.values())\n\ndef process_consolidated_data(consolidated_file):\n    \"\"\"\n    Process consolidated PR data and validate it.\n    Returns (grouped_prs, failing_prs, errors)\n    \"\"\"\n    errors = []\n    valid_prs = []\n    \n    try:\n        with open(consolidated_file) as f:\n            prs = json.load(f)\n    except FileNotFoundError:\n        return None, None, [f\"File {consolidated_file} not found.\"]\n    except json.JSONDecodeError:\n        return None, None, [f\"Error decoding {consolidated_file}.\"]\n    \n    if not isinstance(prs, list):\n        return None, None, [\"Consolidated data must be a list of PRs.\"]\n    \n    # Validate each PR\n    for i, pr in enumerate(prs):\n        is_valid, error = validate_pr_data(pr)\n        if is_valid:\n            valid_prs.append(pr)\n        else:\n            errors.append(f\"PR at index {i}: {error}\")\n    \n    if not valid_prs:\n        return None, None, [\"No valid PRs found in consolidated data.\"]\n    \n    # Group valid PRs by title\n    grouped_prs = group_prs_by_title(valid_prs)\n    \n    # Extract failing PRs\n    failing_prs = [\n        {\n            \"title\": pr[\"title\"],\n            \"url\": f\"https://github.com/{pr['repository']}/pull/{pr['number']}\",\n            \"status\": pr[\"checkStatus\"]\n        }\n        for pr in valid_prs\n        if pr[\"state\"] == \"OPEN\" and pr[\"checkStatus\"] in [\"ERROR\", \"FAILURE\"]\n    ]\n    \n    return grouped_prs, failing_prs, errors\n\n# Main execution\nif len(sys.argv) != 3:\n    print(\"Usage: python3 upload_to_sheets.py <consolidated-prs-json-file> <failing-prs-error-state>\")\n    sys.exit(1)\n\nCONSOLIDATED_FILE = sys.argv[1]\nFAILING_PRS_ERROR = sys.argv[2].lower() == 'true'\n\n# Process consolidated data\ngrouped_prs, failing_prs, errors = process_consolidated_data(CONSOLIDATED_FILE)\n\nif errors:\n    for error in errors:\n        logging.error(error)\n    if not grouped_prs:  # Fatal errors\n        sys.exit(1)\n    else:  # Non-fatal errors\n        logging.warning(\"Some PRs were invalid but processing will continue.\")\n\n# Define the scope\nscope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n\n# Add your service account credentials\n# Check for environment variable first, then fall back to local file\ncredentials_file = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', 'concise-complex-344219-062a255ca56f.json')\ntry:\n    creds = Credentials.from_service_account_file(credentials_file, scopes=scope)\n    logging.info(f\"Using credentials from: {credentials_file}\")\nexcept FileNotFoundError:\n    # For GitHub Actions, try the file we created in the workflow\n    fallback_file = 'google-credentials.json'\n    logging.info(f\"Credentials file {credentials_file} not found, trying {fallback_file}\")\n    creds = Credentials.from_service_account_file(fallback_file, scopes=scope)\n\n# Authorize the client\nclient = gspread.authorize(creds)\n\n# Open the Google Sheet by name or ID\nspreadsheet = client.open(\"Jenkins PR Tracker\")  # or use client.open_by_key(\"YOUR_SHEET_ID\")\n\n# Create a summary sheet\ntry:\n    summary_sheet = spreadsheet.worksheet(\"Summary\")\n    logging.info(\"Summary sheet already exists. Updating it...\")\nexcept gspread.exceptions.WorksheetNotFound:\n    logging.info(\"Creating new Summary sheet...\")\n    summary_sheet = spreadsheet.add_worksheet(title=\"Summary\", rows=100, cols=10)\n\n# Prepare summary data\ntotal_prs = 0\nopen_prs = 0\nclosed_prs = 0\nmerged_prs = 0\nplugin_stats = {}\nearliest_date = None\nlatest_date = None\n\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    total_prs += len(prs)\n    open_prs += pr[\"open\"]\n    closed_prs += pr[\"closed\"]\n    merged_prs += pr[\"merged\"]\n\n    # Plugin-specific stats\n    plugin_stats[title] = {\n        \"total\": len(prs),\n        \"open\": pr[\"open\"],\n        \"closed\": pr[\"closed\"],\n        \"merged\": pr[\"merged\"]\n    }\n\n    # Find the earliest and latest dates\n    for p in prs:\n        created_at = datetime.fromisoformat(p[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n        updated_at = datetime.fromisoformat(p[\"updatedAt\"].replace(\"Z\", \"+00:00\"))\n\n        if earliest_date is None or created_at < earliest_date:\n            earliest_date = created_at\n        if latest_date is None or updated_at > latest_date:\n            latest_date = updated_at\n\n# Calculate percentages\nopen_percentage = (open_prs / total_prs) * 100 if total_prs > 0 else 0\nclosed_percentage = (closed_prs / total_prs) * 100 if total_prs > 0 else 0\nmerged_percentage = (merged_prs / total_prs) * 100 if total_prs > 0 else 0\n\n# Prepare summary data for the sheet\nsummary_data = [\n    [\"PR Date Range\", f\"{earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\", \"\", \"\", \"\", \"\"],\n    [\"Overall PR Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Total PRs\", total_prs, \"\", \"\", \"\", \"\"],\n    [\"Open PRs\", open_prs, f\"{open_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Closed PRs\", closed_prs, f\"{closed_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"Merged PRs\", merged_prs, f\"{merged_percentage:.2f}%\", \"\", \"\", \"\"],\n    [\"\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin-Specific Statistics\", \"\", \"\", \"\", \"\", \"\"],\n    [\"Plugin\", \"Total PRs\", \"Open PRs\", \"Closed PRs\", \"Merged PRs\", \"Link to Sheet\"]\n]\n\n# Add plugin-specific stats and links to individual sheets\nfor plugin, stats in plugin_stats.items():\n    sheet_name = sanitize_sheet_name(plugin)\n    if not sheet_name:\n        logging.error(f\"Invalid sheet name generated for plugin '{plugin}'. Skipping sheet creation.\")\n        continue\n\n    try:\n        plugin_sheet = spreadsheet.worksheet(sheet_name)\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n        plugin_sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n    link = f'=HYPERLINK(\"#gid={plugin_sheet.id}\"; \"{plugin}\")'\n    summary_data.append([\n        plugin,\n        stats[\"total\"],\n        stats[\"open\"],\n        stats[\"closed\"],\n        stats[\"merged\"],\n        link\n    ])\n\n# Update the summary sheet\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Reorder sheets to make the Summary sheet first\nsheets = spreadsheet.worksheets()\nif sheets[0].title != \"Summary\":\n    summary_sheet_index = next((i for i, sheet in enumerate(sheets) if sheet.title == \"Summary\"), None)\n    if summary_sheet_index is not None:\n        spreadsheet.reorder_worksheets(\n            [sheets[summary_sheet_index]] + [sheet for i, sheet in enumerate(sheets) if i != summary_sheet_index])\n\n# Get the Summary sheet ID for the \"Back to Summary\" link\nsummary_sheet_id = summary_sheet.id\n\n# Format the summary sheet\nformat_sheet_with_retry(summary_sheet, \"A1:F1\", {\n    \"textFormat\": {\n        \"bold\": True\n    },\n    \"backgroundColor\": {\n        \"red\": 0.9,  # Light gray background\n        \"green\": 0.9,\n        \"blue\": 0.9,\n        \"alpha\": 1.0\n    },\n    \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n})\n\n# Create a new sheet for failing PRs\nif failing_prs and isinstance(failing_prs, list):\n    try:\n        failing_prs_sheet = spreadsheet.worksheet(\"Failing PRs\")\n        logging.info(\"Failing PRs sheet already exists. Updating it...\")\n    except gspread.exceptions.WorksheetNotFound:\n        logging.info(\"Creating new Failing PRs sheet...\")\n        failing_prs_sheet = spreadsheet.add_worksheet(title=\"Failing PRs\", rows=100, cols=10)\n\n    # Prepare the data for the failing PRs sheet\n    failing_prs_data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Title\", \"URL\", \"Status\"]\n    ]\n\n    # Process each failing PR\n    for pr in failing_prs:\n        failing_prs_data.append([\n            pr[\"title\"],\n            f'=HYPERLINK(\"{pr[\"url\"]}\"; \"{pr[\"url\"]}\")',\n            pr[\"status\"]\n        ])\n\n    # Clear the sheet and update it with the new data\n    update_sheet_with_retry(failing_prs_sheet, failing_prs_data)\n\n    # Format the column titles (bold font and background color)\n    format_sheet_with_retry(failing_prs_sheet, \"A3:C3\", {  # Format only the column titles (row 3)\n        \"textFormat\": {\n            \"bold\": True\n        },\n        \"backgroundColor\": {\n            \"red\": 0.9,  # Light gray background\n            \"green\": 0.9,\n            \"blue\": 0.9,\n            \"alpha\": 1.0\n        },\n        \"horizontalAlignment\": \"CENTER\"  # Center-align the text\n    })\n\n    # Calculate failing PRs count\n    failing_prs_count = len(failing_prs)\nelse:\n    failing_prs_count = 0\n    logging.warning(\"No failing PRs found in the data.\")\n\n# Add a link to the \"Failing PRs\" sheet in the \"Summary\" sheet and include the count\nif failing_prs and 'failing_prs_sheet' in locals():\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", f'=HYPERLINK(\"#gid={failing_prs_sheet.id}\"; \"Failing PRs\")'])\nelse:\n    summary_data.append([\"Failing PRs\", failing_prs_count, \"\", \"\", \"\", \"No failing PRs found\"])\nupdate_sheet_with_retry(summary_sheet, summary_data)\n\n# Iterate through each PR group and create a new sheet for each title\nfor pr in grouped_prs:\n    title = pr[\"title\"]\n    prs = pr[\"prs\"]\n    sheet_name = sanitize_sheet_name(title)\n\n    # Prepare the data for the sheet\n    data = [\n        [\"Back to Summary\", f'=HYPERLINK(\"#gid={summary_sheet_id}\"; \"Back to Summary\")', \"\", \"\", \"\"],\n        [title, \"\", \"\", \"\", \"\"],  # Add title without label\n        [\"\", \"\", \"\", \"\", \"\"],  # Empty row for spacing\n        [\"Repository\", \"PR Number\", \"State\", \"Created At\", \"Updated At\"]\n    ]\n    for p in prs:\n        # Add hyperlinks to the Repository and PR Number columns\n        repo_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}\"; \"{p[\"repository\"]}\")'\n        pr_link = f'=HYPERLINK(\"https://github.com/{p[\"repository\"]}/pull/{p[\"number\"]}\"; \"{p[\"number\"]}\")'\n        data.append([repo_link, pr_link, p[\"state\"], p[\"createdAt\"], p[\"updatedAt\"]])\n\n    try:\n        # Check if a sheet with the same title already exists\n        try:\n            sheet = spreadsheet.worksheet(sheet_name)\n            logging.info(f\"Sheet '{sheet_name}' already exists. Updating it...\")\n        except gspread.exceptions.WorksheetNotFound:\n            # Create a new sheet if it doesn't exist\n            logging.info(f\"Creating new sheet for '{sheet_name}'...\")\n            sheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=10)\n\n        # Update sheet with retry logic\n        update_sheet_with_retry(sheet, data)\n\n        # Format the \"Back to Summary\" row - only format the first cell\n        format_sheet_with_retry(sheet, \"A1\", {\n            \"textFormat\": {\n                \"fontSize\": 10\n            }\n        })\n\n        # Format the title row - only format the first cell\n        format_sheet_with_retry(sheet, \"A2\", {\n            \"textFormat\": {\n                \"bold\": True,\n                \"fontSize\": 10\n            },\n            \"horizontalAlignment\": \"LEFT\"\n        })\n\n        # Format the column titles\n        format_sheet_with_retry(sheet, \"A4:E4\", {\n            \"textFormat\": {\n                \"bold\": True\n            },\n            \"backgroundColor\": {\n                \"red\": 0.9,\n                \"green\": 0.9,\n                \"blue\": 0.9,\n                \"alpha\": 1.0\n            },\n            \"horizontalAlignment\": \"CENTER\"\n        })\n\n        # Apply conditional formatting based on PR state\n        format_requests = []\n        for row_idx, p in enumerate(prs, start=5):  # Start from row 5 (skip header, title, and \"Back to Summary\" rows)\n            color = {\n                \"MERGED\": {\"red\": 0.0, \"green\": 1.0, \"blue\": 0.0, \"alpha\": 1.0},\n                \"OPEN\": {\"red\": 1.0, \"green\": 0.5, \"blue\": 0.0, \"alpha\": 1.0},\n                \"CLOSED\": {\"red\": 1.0, \"green\": 0.0, \"blue\": 0.0, \"alpha\": 1.0},\n            }.get(p[\"state\"], {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0, \"alpha\": 1.0})\n\n            format_requests.append({\n                \"range\": f\"A{row_idx}:E{row_idx}\",\n                \"format\": {\n                    \"backgroundColor\": color\n                }\n            })\n\n        # Apply all formatting requests in a single batch\n        if format_requests:\n            batch_format_with_retry(sheet, format_requests)\n\n        # Add a longer delay between sheets to avoid rate limits\n        time.sleep(10)  # Increased delay between sheets\n\n    except gspread.exceptions.APIError as e:\n        logging.error(f\"Failed to update sheet '{sheet_name}': {e}\")\n        continue\n\nlogging.info(\"Data has been uploaded to Google Sheets.\")\n"
                    }
                ]
            },
            "createdAt": 1743511813681,
            "isInternal": false
        }
    ],
    "isAgent": false,
    "isNameGenerated": true,
    "isCustomAgent": false,
    "isUnitTestsAgent": false,
    "autoApply": false
}
}